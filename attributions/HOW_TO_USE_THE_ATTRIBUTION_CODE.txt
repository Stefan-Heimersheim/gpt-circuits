HOW TO USE THE ATTRIBUTION CODE:

0) Do the usual setup, run the following if you haven't already
pip install -r requirements.txt
python -m data.shakespeare.prepare

1) Open attributions/model_download. This should be the only file you need to open. Put in whatever you want so that:
    model_names is a list of strings pointing to models in huggingface, the models you want to run attributions on
    local_dirs is a list (of the same size) of strings with the name of folders where you want to save the model
    save_names is a list (of the same size) of strings that you want the model attribution to be saved with
    download = True

2) Run model_download. It should save the models to different folders (usually in checkpoints, but you get to control this). It will also make a folder gpt-circuits/ called 'commands.txt'

3) in the terminal run 'bash < commands.txt'
    This should fill attributions/data with safetensor files with the names you listed in save_names

4) In terminal run 'huggingface-cli login' Enter a login code when prompted. 

5) Reopen attributions/model_download. Set download = False, save, and run 'python attributions/model_download' is the terminal

6) You are all done!

Points of concern: 
I have encountered cross transformer block models that do not have a SAE after the 4th layer. This will cause the code to fail. 
Fast fix: go to line 68 in attributor.py and change it to the commented version 'for i in range(layers-1):'



