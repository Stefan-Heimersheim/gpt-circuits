# %%
import os
import sys
# %%
# current_dir = os.path.abspath(os.path.dirname(__file__))
# while os.path.basename(current_dir) != "gpt-circuits":
#     current_dir = os.path.abspath(os.path.join(current_dir, ".."))
# os.chdir(current_dir)
# print(f"Current directory: {current_dir}")

os.environ["CUDA_VISIBLE_DEVICES"] = "1"
# %%
import torch
import numpy as np
from tqdm import tqdm
from config.gpt.models import gpt_options
from config.gpt.training import options as gpt_train_options
from models.gpt import GPT
from models.sae.multilayer import MultiLayerSAEBase
import warnings
import argparse
from config.sae.training import SAETrainingConfig, LossCoefficients
from config.sae.models import SAEConfig, SAEVariant
from jaxtyping import Float
from torch import Tensor
from torch.utils.data import TensorDataset, DataLoader
from david.OLD.cache_activations import cache_activations
device = "cuda" if torch.cuda.is_available() else "cpu"
model_name = 'shakespeare_64x4'
# Add root directory to sys.path dynamically


# Define paths
checkpoints = "checkpoints"
data_root = "data"
model_path = os.path.join(checkpoints, model_name)
dataset_path = os.path.join(data_root, model_name)

# Load model configuration
config = gpt_train_options[model_name]
model = GPT(config.gpt_config)
model = model.load(model_path, device=config.device)
model.to(device)

if not os.path.exists(dataset_path):
    os.makedirs(dataset_path, exist_ok=True)
    cache_activations(dataset_path, model_path, batch_size=64, device=device)

cache_train = np.load(os.path.join(dataset_path, "train_activations.npy"), allow_pickle=False)
cache_val = np.load(os.path.join(dataset_path, "val_activations.npy"), allow_pickle=False)
# %%

# Shared training parameters
shakespeare_64x4_defaults = {
    "data_dir": "data/shakespeare",
    "eval_interval": 250,
    "eval_steps": 100,
    "batch_size": 128,
    "gradient_accumulation_steps": 1,
    "learning_rate": 1e-3,
    "warmup_steps": 750,
    "max_steps": 7500,
    "decay_lr": True,
    "min_lr": 1e-4,
}

starcasex8_options = SAEConfig(
        name="staircasex8.shakespeare_64x4",
        gpt_config=gpt_options["ascii_64x4"],
        n_features=64 * 8,
        sae_variant=SAEVariant.STANDARD,
    )

sae_train_config = SAETrainingConfig(
        name="multilayer.shakespeare_64x4",
        sae_config=starcasex8_options,
        **shakespeare_64x4_defaults,
        loss_coefficients=LossCoefficients(
            sparsity=(0.06, 0.24, 0.8, 1.0, 2.5),  # Targets L0s of ~10
        ))
sae = MultiLayerSAEBase(sae_train_config.sae_config, sae_train_config.loss_coefficients)
sae.to(device)
# compile it
comp_sae = sae.compile()

config = shakespeare_64x4_defaults

train_data = cache_train
val_data = cache_val

# Create datasets and dataloaders
train_tensor = torch.from_numpy(train_data).to(device)
val_tensor = torch.from_numpy(val_data).to(device)

train_dataset = TensorDataset(train_tensor)
val_dataset = TensorDataset(val_tensor)

train_loader = DataLoader(
    train_dataset, 
    batch_size=config['batch_size'],
    shuffle=True,
)

val_loader = DataLoader(
    val_dataset,
    batch_size=config['batch_size'],
    shuffle=False,
)


class ScaledAdamW(torch.optim.AdamW):
    def __init__(self, params, sae, lr=0.001, **kwargs):
        super().__init__(params, lr=lr, **kwargs)
        self.sae = sae
    
    def step(self):
        # Scale gradients
        n_layers = self.sae.n_layers
        feature_size = self.sae.feature_size
        
        with torch.no_grad():
            param_groups = [
                (self.sae.W_dec.grad, lambda idx: (idx*feature_size, (idx+1)*feature_size), lambda slice_idx: (slice(*slice_idx), slice(None))),
                (self.sae.W_enc.grad, lambda idx: (idx*feature_size, (idx+1)*feature_size), lambda slice_idx: (slice(None), slice(*slice_idx))),
                (self.sae.b_enc.grad, lambda idx: (idx*feature_size, (idx+1)*feature_size), lambda slice_idx: (slice(*slice_idx),))
            ]
            
            for grad, slice_fn, index_fn in param_groups:
                if grad is not None:
                    for layer_idx in range(n_layers):
                        slice_idx = slice_fn(layer_idx)
                        scaling_factor = 1.0 / (n_layers - layer_idx)
                        idx = index_fn(slice_idx)
                        grad[idx] *= scaling_factor
        
        # Call parent step
        super().step()

# Usage
optimizer = ScaledAdamW(sae.parameters(), sae, lr=config['learning_rate'])
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=0)


# Learning rate scheduler
if config['decay_lr']:
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=config['max_steps'] - config['warmup_steps'],
        eta_min=config['min_lr']
    )
# %%
# Training loop
sae.train()
best_val_loss = float('inf')
step = 0
optimizer.zero_grad()  # Initialize gradients

progress_bar = tqdm(total=config['max_steps'], desc='Training')

# Learning rate warmup
def get_lr(step, warmup_steps, base_lr, min_lr):
    if step < warmup_steps:
        return base_lr * (step / warmup_steps)
    return base_lr

while step < config['max_steps']:
    for batch in train_loader:
        if step >= config['max_steps']:
            break
            
        # Move data to device
        batch_data = batch[0].to(device)
        
        # Accumulate loss across all layers for this batch
        total_batch_loss = 0
        
        # Process each layer in the SAE sequentially
        for layer_idx in range(sae.n_layers):
            # Forward pass through this layer
            output = sae(batch_data, layer_idx=layer_idx)
            layer_loss = output.loss.total
            
            total_batch_loss += layer_loss
        
        # Now backward pass on accumulated loss with gradient accumulation
        total_batch_loss = total_batch_loss / config['gradient_accumulation_steps']
        total_batch_loss.backward()
        
        # Gradient accumulation and optimization
        if (step + 1) % config['gradient_accumulation_steps'] == 0:
            # Learning rate warmup
            if step < config['warmup_steps']:
                for param_group in optimizer.param_groups:
                    param_group['lr'] = get_lr(
                        step, 
                        config['warmup_steps'], 
                        config['learning_rate'], 
                        config['min_lr']
                    )
            
            optimizer.step()
            optimizer.zero_grad()
            
            # Update learning rate with scheduler after warmup
            if config['decay_lr'] and step >= config['warmup_steps']:
                scheduler.step()
        
                # Evaluation
        if step % config['eval_interval'] == 0:
            sae.eval()
            val_losses = []
            
            # Initialize metrics to track per layer
            layer_metrics = {i: {'l0': 0.0, 'reconstruct': 0.0, 'sparsity': 0.0} for i in range(sae.n_layers)}
            n_batches = 0
            
            with torch.no_grad():
                # Evaluate all layers for each validation batch
                for val_batch in val_loader:
                    val_data = val_batch[0].to(device)
                    n_batches += 1
                    
                    # Accumulate validation loss across all layers
                    batch_val_loss = 0
                    for layer_idx in range(sae.n_layers):
                        val_output = sae(val_data, layer_idx=layer_idx)
                        batch_val_loss += val_output.loss.total.item() / sae.n_layers
                        
                        # Accumulate metrics for this layer
                        layer_metrics[layer_idx]['l0'] += val_output.loss.l0.item()
                        layer_metrics[layer_idx]['reconstruct'] += val_output.loss.reconstruct.item()
                        layer_metrics[layer_idx]['sparsity'] += val_output.loss.sparsity.item()
                    
                    val_losses.append(batch_val_loss)
                    
                    if len(val_losses) >= config['eval_steps']:
                        break
                
                # Calculate average metrics per layer
                for layer_idx in range(sae.n_layers):
                    for metric in layer_metrics[layer_idx]:
                        layer_metrics[layer_idx][metric] /= n_batches
                
                avg_val_loss = sum(val_losses) / len(val_losses)
                
                # Print layer-wise metrics
                print(f"\nStep {step} - Layer-wise metrics:")
                for layer_idx in range(sae.n_layers):
                    print(f"  Layer {layer_idx}: L0={layer_metrics[layer_idx]['l0']:.2f}, "
                        f"Recon={layer_metrics[layer_idx]['reconstruct']:.4f}, "
                        f"Sparsity={layer_metrics[layer_idx]['sparsity']:.4f}")
                
                # Save best model
                if avg_val_loss < best_val_loss:
                    best_val_loss = avg_val_loss
                    torch.save({
                        'step': step,
                        'sae_state_dict': sae.state_dict(),  # Save state dict of original sae as it was compiled
                        'optimizer_state_dict': optimizer.state_dict(),
                        'val_loss': best_val_loss,
                        'layer_metrics': layer_metrics,  # Save metrics in checkpoint
                    }, os.path.join(checkpoints, f"{model_name}_multi_sae_best.pt"))
                
                progress_bar.set_postfix({
                    'lr': optimizer.param_groups[0]['lr'],
                    'val_loss': f'{avg_val_loss:.4f}',
                    'best_val_loss': f'{best_val_loss:.4f}',
                    'avg_l0': f"{sum(m['l0'] for m in layer_metrics.values())/sae.n_layers:.2f}"
                })
            
            sae.train()
        
        step += 1
        progress_bar.update(1)
        
        if step >= config['max_steps']:
            break

progress_bar.close()
#return model

# %%
from safetensors.torch import save_file
import torch
import os
import json
def save_sae_to_huggingface(sae: MultiLayerSAEBase, save_dir: str, model_name: str = "sae"):
    """
    Save the MultiLayerSAEBase model to Hugging Face format using safetensors.
    
    Args:
        sae: The MultiLayerSAEBase instance to save
        save_dir: Directory where the model will be saved
        model_name: Name of the model file (default: "multi_sae")
    """
    # Create save directory if it doesn’t exist
    os.makedirs(save_dir, exist_ok=True)
    
    # Prepare the state dictionary with contiguous tensors
    state_dict = {
        "W_dec": sae.W_dec.contiguous(),
        "W_enc": sae.W_enc.contiguous(),
        "b_enc": sae.b_enc.contiguous(),
        "b_dec": sae.b_dec.contiguous(),
    }
    
    # Save the tensors using safetensors
    save_path = os.path.join(save_dir, f"{model_name}.safetensors")
    save_file(state_dict, save_path)
    
    # Prepare configuration dictionary, converting device to string
    gpt_config_dict = vars(sae.config.gpt_config)  # Convert dataclass to dict
    if "device" in gpt_config_dict and isinstance(gpt_config_dict["device"], torch.device):
        gpt_config_dict["device"] = str(gpt_config_dict["device"])  # Convert torch.device to string
    
    config_dict = {
        "feature_size": sae.feature_size,
        "n_layers": sae.n_layers,
        "gpt_config": gpt_config_dict,
        "l1_coefficient": sae.l1_coefficient if sae.l1_coefficient is not None else None,
    }
    
    # Save configuration as JSON
    with open(os.path.join(save_dir, "config.json"), "w") as f:
        json.dump(config_dict, f, indent=4)
    
    print(f"Model saved to {save_path}")

# Example usage
sae = MultiLayerSAEBase(config=sae_train_config.sae_config, loss_coefficients=sae_train_config.loss_coefficients)
save_dir = "../checkpoints/multi-layer.shakespeare_64x4"
save_sae_to_huggingface(sae, save_dir)
# %%
